{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization techniques\n",
    "\n",
    "# extractive - extract the most important sentences from a document (subset of the actual doc)\n",
    "\n",
    "# abstractive - brief/summarize the actual document in your own words\n",
    "\n",
    "# ntf = base words from all sentences, each word)/(word appeasr most\n",
    "\n",
    "# idf = sentences in doc)/(number of sentences word appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# slip document by of paragraphs\n",
    "def split_into_paragraphs(doc):\n",
    "    paragraphs = doc.split(\"\\n\")\n",
    "    return  paragraphs, len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_sentences_word_appears(word):\n",
    "    count = 0\n",
    "    for i in range(len(sentences_dict)):\n",
    "        for w in range(len(sentences_dict[i])):\n",
    "            if (word == sentences_dict[i][w]):\n",
    "                count += 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ntf_matrix(sentences_dict, no_dup_words):\n",
    "    ntf_matrix = []\n",
    "    for s in range(len(sentences_dict)):\n",
    "        ntf_matrix.append([0])\n",
    "        for i in range(len(no_dup_words)):\n",
    "            ntf_matrix[s].append(sentences_dict[s].count(no_dup_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(sentences_dict):\n",
    "    ranking = {}\n",
    "    for i in range(len(sentences_dict)):\n",
    "        ranking[i] = ( len(intersection_of_two_sentences(sentences_dict[0], sentences_dict[i])) \n",
    "                       / len(union_of_two_sentences(sentences_dict[0], sentences_dict[i]))\n",
    "                      )\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_of_two_sentences(sentence_1, sentence_2):\n",
    "    final = [value for value in sentence_1 if value in sentence_2]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_of_two_sentences(sentence_1, sentence_2):\n",
    "    return sentence_1 + sentence_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem_doc(doc):\n",
    "#     doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take parameters of text file and postive integer\n",
    "\n",
    "\n",
    "def summarize_doc(input_file, N):\n",
    "# Open file\n",
    "# read content\n",
    "    f = open(input_file, \"r\")\n",
    "    content = f.read()\n",
    "    \n",
    "    doc = TextBlob(content)\n",
    "    \n",
    "#     get the paragraphs and the length\n",
    "    paragraphs, parapraph_len = split_into_paragraphs(doc)\n",
    "    \n",
    "#     get all the words in the doc\n",
    "    words = doc.words\n",
    "    sentences = doc.sentences\n",
    "\n",
    "# -----------\n",
    "#     remove stop words\n",
    "#     doc = TextBlob(doc.words)\n",
    "#     stem document\n",
    "# -----------\n",
    "\n",
    "#     IDF for words\n",
    "    idf = {w : 0 for w in words}\n",
    "\n",
    "#     Sentences\n",
    "    sentences_dict = {}\n",
    "    for i in range(len(sentences)):\n",
    "        sentences_dict[i] = sentences[i].words\n",
    "\n",
    "#     remove duplicate words\n",
    "    no_dup_words = list(set(words))\n",
    "\n",
    "#    calculate the idf\n",
    "    for i in range(len(no_dup_words)):\n",
    "        idf[no_dup_words[i]] = math.log((len(sentences_dict)/number_of_sentences_word_appears(no_dup_words[i])))+1\n",
    "    \n",
    "#     ntf matrix\n",
    "    ntf_matrix = create_ntf_matrix(sentences_dict, no_dup_words)\n",
    "    \n",
    "\n",
    "#   Jaccard Similarity Rank\n",
    "    ranking = {}\n",
    "    ranking = rank_sentences(sentences_dict)\n",
    "\n",
    "    \n",
    "#   based off the ranking, return the N top sentences\n",
    "    sorted_rank = dict(sorted(ranking.items(), key=operator.itemgetter(1), reverse=True))\n",
    "\n",
    "#    return the top N ranked sentences\n",
    "    summarized_doc = ''\n",
    "    for j in range(N):\n",
    "        summarized_doc += str(sentences[j]) + ' '   \n",
    "        \n",
    "#   concatinate sentences into a string and add to output file .smz\n",
    "    f = open(\"summarized_document.smz\", \"w\")\n",
    "    \n",
    "#   wite to .smz\n",
    "    f.write(summarized_doc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.07407407407407407, 2: 0.05, 5: 0.04878048780487805, 3: 0.034482758620689655, 4: 0.0, 6: 0.0}\n"
     ]
    }
   ],
   "source": [
    "summarize_doc('input_doc.txt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('input_doc.txt', \"r\")\n",
    "content = f.read()\n",
    "    \n",
    "doc = TextBlob(content)\n",
    "#     get the paragraphs and the length\n",
    "paragraphs, parapraph_len = split_into_paragraphs(doc)\n",
    "    \n",
    "#     get all the words in the doc\n",
    "words = list(set(doc.words))\n",
    "sentences = doc.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {w : 0 for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [words.count(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dict = {}\n",
    "for i in range(len(sentences)):\n",
    "    sentences_dict[i] = sentences[i].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(no_dup_words)):\n",
    "    idf[no_dup_words[i]] = math.log((len(sentences_dict)/number_of_sentences_word_appears(no_dup_words[i])))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---pending-- need to take into account the paragraphs\n",
    "def number_of_sentences_word_appears(word):\n",
    "    count = 0\n",
    "    for i in range(len(sentences_dict)):\n",
    "        for w in range(len(sentences_dict[i])):\n",
    "            if (word == sentences_dict[i][w]):\n",
    "                count += 1\n",
    "    return count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'We': 2.9459101490553135,\n",
       " 'consider': 2.252762968495368,\n",
       " 'summarization': 1.336472236621213,\n",
       " 'in': 2.252762968495368,\n",
       " 'this': 2.252762968495368,\n",
       " 'module': 2.9459101490553135,\n",
       " 'To': 2.9459101490553135,\n",
       " 'facilitate': 2.9459101490553135,\n",
       " 'the': 0.6433250560612676,\n",
       " 'discussion': 2.9459101490553135,\n",
       " 'we': 2.9459101490553135,\n",
       " 'of': 0.8664686073754774,\n",
       " 'a': 1.1541506798272583,\n",
       " 'single': 2.9459101490553135,\n",
       " 'given': 2.252762968495368,\n",
       " 'document': 1.336472236621213,\n",
       " 'There': 2.9459101490553135,\n",
       " 'are': 2.9459101490553135,\n",
       " 'two': 2.252762968495368,\n",
       " 'modes': 2.9459101490553135,\n",
       " 'extractive': 1.8472978603872037,\n",
       " 'and': 1.8472978603872037,\n",
       " 'abstractive': 2.252762968495368,\n",
       " 'An': 2.252762968495368,\n",
       " 'is': 2.252762968495368,\n",
       " 'subset': 2.9459101490553135,\n",
       " 'original': 2.252762968495368,\n",
       " 'In': 2.9459101490553135,\n",
       " 'other': 2.9459101490553135,\n",
       " 'words': 2.9459101490553135,\n",
       " 'an': 2.252762968495368,\n",
       " 'summarizer': 2.252762968495368,\n",
       " 'attempts': 2.9459101490553135,\n",
       " 'to': 2.252762968495368,\n",
       " 'determine': 2.9459101490553135,\n",
       " 'most': 2.9459101490553135,\n",
       " 'important': 2.9459101490553135,\n",
       " 'sentences': 1.8472978603872037,\n",
       " 'within': 2.9459101490553135,\n",
       " 'presents': 2.9459101490553135,\n",
       " 'these': 2.9459101490553135,\n",
       " 'as': 2.9459101490553135,\n",
       " 'provides': 2.9459101490553135,\n",
       " 'abstract': 2.252762968495368,\n",
       " 'You': 2.9459101490553135,\n",
       " 'might': 2.9459101490553135,\n",
       " 'think': 2.9459101490553135,\n",
       " 'it': 2.9459101490553135,\n",
       " 'way': 2.9459101490553135,\n",
       " 'highly': 2.252762968495368,\n",
       " 'trained': 2.9459101490553135,\n",
       " 'professional': 2.9459101490553135,\n",
       " 'asked': 2.9459101490553135,\n",
       " 'read': 2.9459101490553135,\n",
       " 'technical': 2.9459101490553135,\n",
       " 'paper': 1.8472978603872037,\n",
       " 'his/her': 2.9459101490553135,\n",
       " 'specialty': 2.9459101490553135,\n",
       " 'field': 2.9459101490553135,\n",
       " 'provide': 2.9459101490553135,\n",
       " 'short': 2.9459101490553135,\n",
       " 'one': 2.9459101490553135,\n",
       " 'or': 2.9459101490553135,\n",
       " 'paragraph': 2.9459101490553135,\n",
       " 'description': 2.252762968495368,\n",
       " 'This': 2.9459101490553135,\n",
       " 'often': 2.9459101490553135,\n",
       " 'called': 2.9459101490553135,\n",
       " 'need': 2.9459101490553135,\n",
       " 'not': 2.9459101490553135,\n",
       " 'contain': 2.9459101490553135,\n",
       " 'any': 2.9459101490553135,\n",
       " 'from': 2.9459101490553135}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: WordList(['We', 'consider', 'summarization', 'in', 'this', 'module']),\n",
       " 1: WordList(['To', 'facilitate', 'the', 'discussion', 'we', 'consider', 'summarization', 'of', 'a', 'single', 'given', 'document', 'There', 'are', 'two', 'modes', 'of', 'summarization', 'extractive', 'and', 'abstractive']),\n",
       " 2: WordList(['An', 'extractive', 'summarization', 'of', 'a', 'given', 'document', 'is', 'a', 'subset', 'of', 'the', 'original', 'document']),\n",
       " 3: WordList(['In', 'other', 'words', 'an', 'extractive', 'summarizer', 'attempts', 'to', 'determine', 'the', 'most', 'important', 'sentences', 'within', 'the', 'document', 'and', 'presents', 'these', 'sentences', 'as', 'the', 'summarization']),\n",
       " 4: WordList(['An', 'abstractive', 'summarizer', 'provides', 'an', 'abstract', 'of', 'the', 'document']),\n",
       " 5: WordList(['You', 'might', 'think', 'of', 'it', 'this', 'way', 'a', 'highly', 'trained', 'professional', 'is', 'asked', 'to', 'read', 'a', 'highly', 'technical', 'paper', 'in', 'his/her', 'specialty', 'field', 'and', 'provide', 'a', 'short', 'one', 'or', 'two', 'paragraph', 'description', 'of', 'the', 'paper']),\n",
       " 6: WordList(['This', 'description', 'often', 'called', 'the', 'abstract', 'need', 'not', 'contain', 'any', 'of', 'the', 'sentences', 'from', 'the', 'original', 'paper'])}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntf_matrix = []\n",
    "for s in range(len(sentences_dict)):\n",
    "    ntf_matrix.append([0])\n",
    "    for i in range(len(no_dup_words)):\n",
    "        ntf_matrix[s].append(sentences_dict[s].count(no_dup_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ntf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_words = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = []\n",
    "for i in range(len(words)):\n",
    "    count.append(words.count(words[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 10,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 8,\n",
       " 10,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 10,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 10,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 2,\n",
       " 3]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = {}\n",
    "for i in range(len(sentences_dict)):\n",
    "    ranking[i] = (len(intersection_of_two_sentences(sentences_dict[0], sentences_dict[i]))/len(union_of_two_sentences(sentences_dict[0], sentences_dict[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5,\n",
       " 1: 0.07407407407407407,\n",
       " 2: 0.05,\n",
       " 3: 0.034482758620689655,\n",
       " 4: 0.0,\n",
       " 5: 0.04878048780487805,\n",
       " 6: 0.0}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "sorted_d = dict(sorted(ranking.items(), key=operator.itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5,\n",
       " 1: 0.07407407407407407,\n",
       " 2: 0.05,\n",
       " 5: 0.04878048780487805,\n",
       " 3: 0.034482758620689655,\n",
       " 4: 0.0,\n",
       " 6: 0.0}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-195-85f5d7c8643a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-195-85f5d7c8643a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sentences[sorted_d.keys()\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "sentences[sorted_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [i for i in sorted_d.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 5, 3, 4, 6]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_doc = ''\n",
    "for j in range(3):\n",
    "    summarized_doc += str(sentences[j]) + \" \"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We consider summarization in this module. To facilitate the discussion, we consider summarization of a single given document There are two modes of summarization: extractive and abstractive. An extractive summarization of a given document is a subset of the original document. '"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
